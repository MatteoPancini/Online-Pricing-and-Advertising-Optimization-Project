{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts and their generation scenario 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case in which there are three classes of users (C1, C2, and C3), and no information about the advertising and pricing curves is known beforehand. Consider two scenarios. \n",
    "\n",
    "In the first one, the structure of the contexts is known beforehand. Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "In the second scenario, the structure of the contexts is not known beforehand and needs to be learnt from data. \n",
    "\n",
    "Important remark: the learner does not know how many contexts there are, while it can only observe the features and data associated with the features. \n",
    "\n",
    "Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves paired with a context generation algorithm, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "Apply the context generation algorithms every two weeks of the simulation. \n",
    "Compare the performance of the two algorithms --- the one used in the first scenario with the one used in the second scenario. \n",
    "\n",
    "Furthermore, in the second scenario, run the GP-UCB and GP-TS algorithms without context generation, and therefore forcing the context to be only one for the entire time horizon, and compare their performance with the performance of the previous algorithms used for the second scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.User_Classes import UserClass\n",
    "from p4.Multi_TS_Learner import Multi_TS_Learner\n",
    "from p4.Multi_UCB_Learner import Multi_UCB_Learner\n",
    "from p4.class_joint_environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")\n",
    "uc2 = UserClass(name = \"C2\")\n",
    "uc3 = UserClass(name = \"C3\")\n",
    "\n",
    "user_classes = [uc1, uc2, uc3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "#Creating the pricing environment\n",
    "prices = [50, 100, 150, 200, 250]\n",
    "\n",
    "#Defining adv variables \n",
    "n_arms = 100\n",
    "min_bid = 0.01\n",
    "max_bid = 3.0\n",
    "bids = np.linspace(min_bid, max_bid, n_arms)\n",
    "sigma = 200\n",
    "\n",
    "environments = [Environment(uc1), Environment(uc2), Environment(uc3)]\n",
    "\n",
    "optimal_prices = [200, 200, 150]\n",
    "sum_optimal_prices = sum(optimal_prices)\n",
    " \n",
    "T = 100\n",
    "\n",
    "n_experiments = 2\n",
    "\n",
    "ts_rewards_per_experiment = [] #list to store the collected rewards for TS_Learner over each experiment\n",
    "ucb_rewards_per_experiment = [] #list to store the collected rewards for Greedy_Learner over each experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 experiments started\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TS_Learner.update() missing 2 required positional arguments: 'beta' and 'reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation Scenario 1.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation%20Scenario%201.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         pulled_prices_arm \u001b[39m=\u001b[39m pulled_arms[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation%20Scenario%201.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         round_reward \u001b[39m=\u001b[39m environments[user_class]\u001b[39m.\u001b[39mround(pulled_bids_arm, pulled_prices_arm)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation%20Scenario%201.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         multi_UCB_learner[user_class]\u001b[39m.\u001b[39;49mupdate(pulled_bids_arm, pulled_prices_arm, \u001b[39m*\u001b[39;49mround_reward)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation%20Scenario%201.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m ts_rewards_per_experiment\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(multi_TS_learner[x]\u001b[39m.\u001b[39mcollected_rewards \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(user_classes))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ema/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/4_Contexts_and_their_generation%20Scenario%201.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ucb_rewards_per_experiment\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(multi_UCB_learner[x]\u001b[39m.\u001b[39mcollected_rewards \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(user_classes))))\n",
      "File \u001b[0;32m~/Desktop/OLA_2023_Private/Project-Pricing-Advertising-2022-2023/p4/Multi_UCB_Learner.py:28\u001b[0m, in \u001b[0;36mMulti_UCB_Learner.update\u001b[0;34m(self, pulled_bids_arm, pulled_prices_arm, n_conversions, n_clicks, cum_cost, reward)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_click_learner\u001b[39m.\u001b[39mupdate(pulled_bids_arm, n_clicks)\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcum_cost_learner\u001b[39m.\u001b[39mupdate(pulled_bids_arm, cum_cost)\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprice_learner\u001b[39m.\u001b[39;49mupdate(pulled_prices_arm, [n_conversions, n_clicks \u001b[39m-\u001b[39;49m n_conversions, reward])\n",
      "\u001b[0;31mTypeError\u001b[0m: TS_Learner.update() missing 2 required positional arguments: 'beta' and 'reward'"
     ]
    }
   ],
   "source": [
    "#for each experiment, for each day, for each class\n",
    "for e in range(n_experiments):\n",
    "    multi_TS_learner = [Multi_TS_Learner(bids, prices), Multi_TS_Learner(bids, prices), Multi_TS_Learner(bids, prices)]\n",
    "    multi_UCB_learner = [Multi_UCB_Learner(bids, prices), Multi_UCB_Learner(bids, prices), Multi_UCB_Learner(bids, prices)]\n",
    "\n",
    "\n",
    "    for t in range(T):\n",
    "        if t % 10 == 0:\n",
    "            print(f\"{t} experiments started\")\n",
    "        for user_class in range(len(user_classes)):\n",
    "            #TS\n",
    "            pulled_arms = multi_TS_learner[user_class].pull_arms()\n",
    "            pulled_bids_arm = pulled_arms[0]\n",
    "            pulled_prices_arm = pulled_arms[1]\n",
    "            round_reward = environments[user_class].round(pulled_bids_arm, pulled_prices_arm)\n",
    "            multi_TS_learner[user_class].update(pulled_bids_arm, pulled_prices_arm, *round_reward)\n",
    "\n",
    "\n",
    "            # UCB\n",
    "            pulled_arms = multi_UCB_learner[user_class].pull_arms()\n",
    "            pulled_bids_arm = pulled_arms[0]\n",
    "            pulled_prices_arm = pulled_arms[1]\n",
    "            round_reward = environments[user_class].round(pulled_bids_arm, pulled_prices_arm)\n",
    "            multi_UCB_learner[user_class].update(pulled_bids_arm, pulled_prices_arm, *round_reward)\n",
    "\n",
    "    ts_rewards_per_experiment.append(sum(multi_TS_learner[x].collected_rewards for x in range(len(user_classes))))\n",
    "    ucb_rewards_per_experiment.append(sum(multi_UCB_learner[x].collected_rewards for x in range(len(user_classes))))\n",
    "\n",
    "mean_cum_reward_ts = np.mean(ts_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ts = np.std(ts_rewards_per_experiment, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts - std_cum_reward_ts, mean_cum_reward_ts + std_cum_reward_ts, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts)), np.cumsum(mean_cum_reward_ts) - std_cum_reward_ts, np.cumsum(mean_cum_reward_ts) + std_cum_reward_ts, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: mettere a posto gli optima\n",
    "optimum_price = 200\n",
    "optimum_bid = 1.0\n",
    "opt = (200*140*0.5-140) * 3\n",
    "#optimum_price, optimum_bid = get_optimal_parameters(uc1.user_index)\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_inst_regret_ts = opt - np.mean(np.array(ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
