{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts and their generation scenario 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case in which there are three classes of users (C1, C2, and C3), and no information about the advertising and pricing curves is known beforehand. Consider two scenarios. \n",
    "\n",
    "In the first one, the structure of the contexts is known beforehand. Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "In the second scenario, the structure of the contexts is not known beforehand and needs to be learnt from data. \n",
    "\n",
    "Important remark: the learner does not know how many contexts there are, while it can only observe the features and data associated with the features. \n",
    "\n",
    "Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves paired with a context generation algorithm, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "Apply the context generation algorithms every two weeks of the simulation. \n",
    "Compare the performance of the two algorithms --- the one used in the first scenario with the one used in the second scenario. \n",
    "\n",
    "Furthermore, in the second scenario, run the GP-UCB and GP-TS algorithms without context generation, and therefore forcing the context to be only one for the entire time horizon, and compare their performance with the performance of the previous algorithms used for the second scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.User_Classes import UserClass\n",
    "from p4.Multi_TS_Learner import Multi_TS_Learner\n",
    "from p4.Multi_UCB_Learner import Multi_UCB_Learner\n",
    "from p4.class_joint_environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")\n",
    "uc2 = UserClass(name = \"C2\")\n",
    "uc3 = UserClass(name = \"C3\")\n",
    "\n",
    "user_classes = [uc1, uc2, uc3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "#Creating the pricing environment\n",
    "prices = [50, 100, 150, 200, 250]\n",
    "\n",
    "#Defining adv variables \n",
    "n_arms = 100\n",
    "min_bid = 0.01\n",
    "max_bid = 3.0\n",
    "bids = np.linspace(min_bid, max_bid, n_arms)\n",
    "sigma = 200\n",
    "\n",
    "environments = [Environment(uc1), Environment(uc2), Environment(uc3)]\n",
    "\n",
    "optimal_prices = [200, 200, 150]\n",
    "sum_optimal_prices = sum(optimal_prices)\n",
    " \n",
    "T = 100\n",
    "\n",
    "n_experiments = 2\n",
    "\n",
    "ts_rewards_per_experiment = [] #list to store the collected rewards for TS_Learner over each experiment\n",
    "ucb_rewards_per_experiment = [] #list to store the collected rewards for Greedy_Learner over each experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 experiments started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 experiments started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m pulled_prices_arm \u001b[39m=\u001b[39m pulled_arms[\u001b[39m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m round_reward \u001b[39m=\u001b[39m environments[user_class]\u001b[39m.\u001b[39mround(pulled_bids_arm, pulled_prices_arm)\n\u001b[1;32m---> 16\u001b[0m multi_TS_learner[user_class]\u001b[39m.\u001b[39;49mupdate(pulled_bids_arm, pulled_prices_arm, \u001b[39m*\u001b[39;49mround_reward)\n\u001b[0;32m     19\u001b[0m \u001b[39m# UCB\u001b[39;00m\n\u001b[0;32m     20\u001b[0m pulled_arms \u001b[39m=\u001b[39m multi_UCB_learner[user_class]\u001b[39m.\u001b[39mpull_arms()\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\Multi_TS_Learner.py:27\u001b[0m, in \u001b[0;36mMulti_TS_Learner.update\u001b[1;34m(self, pulled_bids_arm, pulled_prices_arm, n_conversions, n_clicks, cum_cost, reward)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, pulled_bids_arm, pulled_prices_arm, n_conversions, n_clicks, cum_cost, reward):\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_observations(reward) \u001b[39m#saves the reward\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_click_learner\u001b[39m.\u001b[39;49mupdate(pulled_bids_arm, n_clicks) \u001b[39m#update pricing\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcum_cost_learner\u001b[39m.\u001b[39mupdate(pulled_bids_arm, cum_cost) \u001b[39m#update advertising\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprice_learner\u001b[39m.\u001b[39mupdate(pulled_prices_arm, n_conversions, n_clicks \u001b[39m-\u001b[39m n_conversions, reward)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\GPTS_learner.py:31\u001b[0m, in \u001b[0;36mGPTS_Learner.update\u001b[1;34m(self, pulled_arm, reward)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_observations(pulled_arm, reward)\n\u001b[1;32m---> 31\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_model()\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\GPTS_learner.py:24\u001b[0m, in \u001b[0;36mGPTS_Learner.update_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpulled_arms)\u001b[39m.\u001b[39mT\n\u001b[0;32m     23\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollected_rewards\n\u001b[1;32m---> 24\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgp\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgp\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39matleast_2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marms)\u001b[39m.\u001b[39mT, return_std\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas, \u001b[39m1e-2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:304\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_restarts_optimizer):\n\u001b[0;32m    302\u001b[0m         theta_initial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng\u001b[39m.\u001b[39muniform(bounds[:, \u001b[39m0\u001b[39m], bounds[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m    303\u001b[0m         optima\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 304\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constrained_optimization(obj_func, theta_initial, bounds)\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m \u001b[39m# Select result from run with minimal (negative) log-marginal\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# likelihood\u001b[39;00m\n\u001b[0;32m    308\u001b[0m lml_values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(itemgetter(\u001b[39m1\u001b[39m), optima))\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:622\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constrained_optimization\u001b[39m(\u001b[39mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[0;32m    621\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfmin_l_bfgs_b\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 622\u001b[0m         opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    623\u001b[0m             obj_func,\n\u001b[0;32m    624\u001b[0m             initial_theta,\n\u001b[0;32m    625\u001b[0m             method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    626\u001b[0m             jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    627\u001b[0m             bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[0;32m    628\u001b[0m         )\n\u001b[0;32m    629\u001b[0m         _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res)\n\u001b[0;32m    630\u001b[0m         theta_opt, func_min \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:692\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    690\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 692\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    693\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    694\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    695\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    696\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:276\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(theta, eval_gradient\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 276\u001b[0m         lml, grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_marginal_likelihood(\n\u001b[0;32m    277\u001b[0m             theta, eval_gradient\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, clone_kernel\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    278\u001b[0m         )\n\u001b[0;32m    279\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlml, \u001b[39m-\u001b[39mgrad\n\u001b[0;32m    280\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:593\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    591\u001b[0m inner_term \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mik,jk->ijk\u001b[39m\u001b[39m\"\u001b[39m, alpha, alpha)\n\u001b[0;32m    592\u001b[0m \u001b[39m# compute K^-1 of shape (n_samples, n_samples)\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m K_inv \u001b[39m=\u001b[39m cho_solve(\n\u001b[0;32m    594\u001b[0m     (L, GPR_CHOLESKY_LOWER), np\u001b[39m.\u001b[39;49meye(K\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), check_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    595\u001b[0m )\n\u001b[0;32m    596\u001b[0m \u001b[39m# create a new axis to use broadcasting between inner_term and\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m# K_inv\u001b[39;00m\n\u001b[0;32m    598\u001b[0m inner_term \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m K_inv[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:208\u001b[0m, in \u001b[0;36mcho_solve\u001b[1;34m(c_and_lower, b, overwrite_b, check_finite)\u001b[0m\n\u001b[0;32m    205\u001b[0m overwrite_b \u001b[39m=\u001b[39m overwrite_b \u001b[39mor\u001b[39;00m _datacopied(b1, b)\n\u001b[0;32m    207\u001b[0m potrs, \u001b[39m=\u001b[39m get_lapack_funcs((\u001b[39m'\u001b[39m\u001b[39mpotrs\u001b[39m\u001b[39m'\u001b[39m,), (c, b1))\n\u001b[1;32m--> 208\u001b[0m x, info \u001b[39m=\u001b[39m potrs(c, b1, lower\u001b[39m=\u001b[39;49mlower, overwrite_b\u001b[39m=\u001b[39;49moverwrite_b)\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39millegal value in \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mth argument of internal potrs\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    211\u001b[0m                      \u001b[39m%\u001b[39m \u001b[39m-\u001b[39minfo)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for each experiment, for each day, for each class\n",
    "for e in range(n_experiments):\n",
    "    multi_TS_learner = [Multi_TS_Learner(bids, prices), Multi_TS_Learner(bids, prices), Multi_TS_Learner(bids, prices)]\n",
    "    multi_UCB_learner = [Multi_UCB_Learner(bids, prices), Multi_UCB_Learner(bids, prices), Multi_UCB_Learner(bids, prices)]\n",
    "\n",
    "\n",
    "    for t in range(T):\n",
    "        if t % 10 == 0:\n",
    "            print(f\"{t} experiments started\")\n",
    "        for user_class in range(len(user_classes)):\n",
    "            #TS\n",
    "            pulled_arms = multi_TS_learner[user_class].pull_arms()\n",
    "            pulled_bids_arm = pulled_arms[0]\n",
    "            pulled_prices_arm = pulled_arms[1]\n",
    "            round_reward = environments[user_class].round(pulled_bids_arm, pulled_prices_arm)\n",
    "            multi_TS_learner[user_class].update(pulled_bids_arm, pulled_prices_arm, *round_reward)\n",
    "\n",
    "\n",
    "            # UCB\n",
    "            pulled_arms = multi_UCB_learner[user_class].pull_arms()\n",
    "            pulled_bids_arm = pulled_arms[0]\n",
    "            pulled_prices_arm = pulled_arms[1]\n",
    "            round_reward = environments[user_class].round(pulled_bids_arm, pulled_prices_arm)\n",
    "            multi_UCB_learner[user_class].update(pulled_bids_arm, pulled_prices_arm, *round_reward)\n",
    "\n",
    "    ts_rewards_per_experiment.append(sum(multi_TS_learner[x].collected_rewards for x in range(len(user_classes))))\n",
    "    ucb_rewards_per_experiment.append(sum(multi_UCB_learner[x].collected_rewards for x in range(len(user_classes))))\n",
    "\n",
    "mean_cum_reward_ts = np.mean(ts_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ts = np.std(ts_rewards_per_experiment, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts - std_cum_reward_ts, mean_cum_reward_ts + std_cum_reward_ts, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts)), np.cumsum(mean_cum_reward_ts) - std_cum_reward_ts, np.cumsum(mean_cum_reward_ts) + std_cum_reward_ts, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clairvoyant_tools import get_optimal_parameters\n",
    "optimum_price1, optimum_bid1, opt1 = get_optimal_parameters(uc1.user_index)\n",
    "optimum_price2, optimum_bid2, opt2 = get_optimal_parameters(uc2.user_index)\n",
    "optimum_price3, optimum_bid3, opt3 = get_optimal_parameters(uc3.user_index)\n",
    "opt = opt1 + opt2 + opt3\n",
    "\n",
    "#optimum_price, optimum_bid = get_optimal_parameters(uc1.user_index)\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_inst_regret_ts = opt - np.mean(np.array(ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
