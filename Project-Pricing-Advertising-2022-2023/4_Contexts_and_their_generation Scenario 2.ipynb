{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts and their generation\n",
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.User_Classes import UserClass\n",
    "from p4.Multi_TS_Learner import Multi_TS_Learner\n",
    "from p4.Multi_UCB_Learner import Multi_UCB_Learner\n",
    "\n",
    "from p4.feature_based_environment import Environment\n",
    "from p4.ContextManager import ContextManager, ContextOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")\n",
    "uc2 = UserClass(name = \"C2\")\n",
    "uc3 = UserClass(name = \"C3\")\n",
    "\n",
    "user_classes = [uc1, uc2, uc3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of experiment 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 of experiment 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 of experiment 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m pulled_arms_per_feature\u001b[39m.\u001b[39mkeys(): \u001b[39m#.keys ritorna 00 01 10 11\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     optimizer_update_input[feature] \u001b[39m=\u001b[39m pulled_arms_per_feature[feature] \u001b[39m+\u001b[39m envs[feature]\u001b[39m.\u001b[39mround(\u001b[39m*\u001b[39mpulled_arms_per_feature[feature]) \u001b[39m#ad ogni contesto assegno l'arm pullata sommata al round (perché la somma al round?)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m context_TS\u001b[39m.\u001b[39;49mupdate(optimizer_update_input) \u001b[39m#e alla fine faccio l'update\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m#context-generation using UCB\u001b[39;00m\n\u001b[0;32m     33\u001b[0m pulled_arms_per_feature \u001b[39m=\u001b[39m context_UCB\u001b[39m.\u001b[39mpull_arms()\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\ContextManager.py:196\u001b[0m, in \u001b[0;36mContextOptimizer.update\u001b[1;34m(self, input_per_feature)\u001b[0m\n\u001b[0;32m    194\u001b[0m     reward_per_context \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(input_per_feature[feature][\u001b[39m5\u001b[39m] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m context)\n\u001b[0;32m    195\u001b[0m     \u001b[39m#now we update the learner for this context\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext_wise_learner[context]\u001b[39m.\u001b[39;49mupdate(pulled_bids_arm, pulled_prices_arm, n_conversion_per_context, n_clicks_per_context, cum_cost_per_context, reward_per_context)\n\u001b[0;32m    197\u001b[0m \u001b[39m#and we collect the reward for each context. The total reward is just the sum\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollected_rewards\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(input_per_feature[feature][\u001b[39m5\u001b[39m] \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m00\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m01\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m10\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m11\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\Multi_TS_Learner.py:28\u001b[0m, in \u001b[0;36mMulti_TS_Learner.update\u001b[1;34m(self, pulled_bids_arm, pulled_prices_arm, n_conversions, n_clicks, cum_cost, reward)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_observations(reward) \u001b[39m#saves the reward\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_click_learner\u001b[39m.\u001b[39mupdate(pulled_bids_arm, n_clicks) \u001b[39m#update pricing\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcum_cost_learner\u001b[39m.\u001b[39;49mupdate(pulled_bids_arm, cum_cost) \u001b[39m#update advertising\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprice_learner\u001b[39m.\u001b[39mupdate(pulled_prices_arm, n_conversions, n_clicks \u001b[39m-\u001b[39m n_conversions, reward)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\GPTS_learner.py:31\u001b[0m, in \u001b[0;36mGPTS_Learner.update\u001b[1;34m(self, pulled_arm, reward)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_observations(pulled_arm, reward)\n\u001b[1;32m---> 31\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_model()\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\p4\\GPTS_learner.py:24\u001b[0m, in \u001b[0;36mGPTS_Learner.update_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpulled_arms)\u001b[39m.\u001b[39mT\n\u001b[0;32m     23\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollected_rewards\n\u001b[1;32m---> 24\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgp\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgp\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39matleast_2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marms)\u001b[39m.\u001b[39mT, return_std\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas, \u001b[39m1e-2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:286\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    283\u001b[0m \u001b[39m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[0;32m    284\u001b[0m optima \u001b[39m=\u001b[39m [\n\u001b[0;32m    285\u001b[0m     (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constrained_optimization(\n\u001b[0;32m    287\u001b[0m             obj_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mtheta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mbounds\n\u001b[0;32m    288\u001b[0m         )\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m ]\n\u001b[0;32m    292\u001b[0m \u001b[39m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# theta\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_restarts_optimizer \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:622\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constrained_optimization\u001b[39m(\u001b[39mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[0;32m    621\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfmin_l_bfgs_b\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 622\u001b[0m         opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    623\u001b[0m             obj_func,\n\u001b[0;32m    624\u001b[0m             initial_theta,\n\u001b[0;32m    625\u001b[0m             method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    626\u001b[0m             jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    627\u001b[0m             bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[0;32m    628\u001b[0m         )\n\u001b[0;32m    629\u001b[0m         _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res)\n\u001b[0;32m    630\u001b[0m         theta_opt, func_min \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:692\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    689\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    690\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 692\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    693\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    694\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    695\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    696\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:276\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(theta, eval_gradient\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 276\u001b[0m         lml, grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_marginal_likelihood(\n\u001b[0;32m    277\u001b[0m             theta, eval_gradient\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, clone_kernel\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    278\u001b[0m         )\n\u001b[0;32m    279\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlml, \u001b[39m-\u001b[39mgrad\n\u001b[0;32m    280\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:553\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    551\u001b[0m K[np\u001b[39m.\u001b[39mdiag_indices_from(K)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\n\u001b[0;32m    552\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     L \u001b[39m=\u001b[39m cholesky(K, lower\u001b[39m=\u001b[39;49mGPR_CHOLESKY_LOWER, check_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    554\u001b[0m \u001b[39mexcept\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mLinAlgError:\n\u001b[0;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39mzeros_like(theta)) \u001b[39mif\u001b[39;00m eval_gradient \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\miniconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:45\u001b[0m, in \u001b[0;36mcholesky\u001b[1;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLAPACK reported an illegal value in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-th argument\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     41\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mon entry to \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPOTRF\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m-\u001b[39minfo))\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m c, lower\n\u001b[1;32m---> 45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcholesky\u001b[39m(a, lower\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, overwrite_a\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, check_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     46\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     c, lower \u001b[39m=\u001b[39m _cholesky(a, lower\u001b[39m=\u001b[39mlower, overwrite_a\u001b[39m=\u001b[39moverwrite_a, clean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     89\u001b[0m                          check_finite\u001b[39m=\u001b[39mcheck_finite)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "envs = {feature: Environment(feature) for feature in ['00', '01', '10', '11']}\n",
    "opts = {feature: envs[feature].optimal for feature in ['00', '01', '10', '11']}\n",
    "n_experiments = 2\n",
    "T = 100\n",
    "prices = [50, 100, 150, 200, 250]\n",
    "bids = np.linspace(0.01, 3.0, 100)\n",
    "\n",
    "no_context_ts_rewards_per_experiment = []\n",
    "context_ts_rewards_per_experiment = []\n",
    "\n",
    "no_context_ucb_rewards_per_experiment = []\n",
    "context_ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    context_TS = ContextOptimizer(Multi_TS_Learner)\n",
    "    nocontext_TS = Multi_TS_Learner(bids, prices)\n",
    "\n",
    "    context_UCB = ContextOptimizer(Multi_UCB_Learner)\n",
    "    nocontext_UCB = Multi_UCB_Learner(bids, prices)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        if t % 10 == 0:\n",
    "            print(f\"{t} of experiment {e}\")\n",
    "\n",
    "        #context-generation using TS\n",
    "        pulled_arms_per_feature = context_TS.pull_arms()\n",
    "        optimizer_update_input = {}\n",
    "        for feature in pulled_arms_per_feature.keys(): #.keys ritorna 00 01 10 11\n",
    "            optimizer_update_input[feature] = pulled_arms_per_feature[feature] + envs[feature].round(*pulled_arms_per_feature[feature]) #ad ogni contesto assegno l'arm pullata sommata al round (perché la somma al round?)\n",
    "        context_TS.update(optimizer_update_input) #e alla fine faccio l'update\n",
    "\n",
    "        #context-generation using UCB\n",
    "        pulled_arms_per_feature = context_UCB.pull_arms()\n",
    "        optimizer_update_input = {}\n",
    "        for feature in pulled_arms_per_feature.keys():\n",
    "            optimizer_update_input[feature] = pulled_arms_per_feature[feature] + envs[feature].round(*pulled_arms_per_feature[feature])\n",
    "        context_UCB.update(optimizer_update_input)\n",
    "\n",
    "\n",
    "        #TS with no context generation\n",
    "        pulled_arms = nocontext_TS.pull_arms()\n",
    "        total_conversions = 0\n",
    "        total_n_clicks = 0\n",
    "        total_cum_cost = 0\n",
    "        total_reward = 0\n",
    "        for feature in ['00', '01', '10', '11']:\n",
    "            n_conversions, n_clicks, cum_cost, reward = envs[feature].round(*pulled_arms)\n",
    "            total_conversions += n_conversions\n",
    "            total_n_clicks += n_clicks\n",
    "            total_cum_cost += cum_cost\n",
    "            total_reward += reward\n",
    "        nocontext_TS.update(*pulled_arms, total_conversions, total_n_clicks, total_cum_cost, total_reward)\n",
    "\n",
    "        # UCB\n",
    "        pulled_arms = nocontext_UCB.pull_arms()\n",
    "        total_conversions = 0\n",
    "        total_n_clicks = 0\n",
    "        total_cum_cost = 0\n",
    "        total_reward = 0\n",
    "        for feature in ['00', '01', '10', '11']:\n",
    "            n_conversions, n_clicks, cum_cost, reward = envs[feature].round(*pulled_arms)\n",
    "            total_conversions += n_conversions\n",
    "            total_n_clicks += n_clicks\n",
    "            total_cum_cost += cum_cost\n",
    "            total_reward += reward\n",
    "        nocontext_UCB.update(*pulled_arms, total_conversions, total_n_clicks, total_cum_cost, total_reward)\n",
    "\n",
    "    context_ts_rewards_per_experiment.append(context_TS.collected_rewards)\n",
    "    no_context_ts_rewards_per_experiment.append(nocontext_TS.collected_rewards)\n",
    "\n",
    "    context_ucb_rewards_per_experiment.append(context_UCB.collected_rewards)\n",
    "    no_context_ucb_rewards_per_experiment.append(nocontext_UCB.collected_rewards)\n",
    "\n",
    "mean_cum_reward_ts_context = np.mean(context_ts_rewards_per_experiment, axis=0)\n",
    "mean_cum_reward_ts_no_context = np.mean(no_context_ts_rewards_per_experiment, axis=0)\n",
    "\n",
    "std_cum_reward_ts_context = np.std(context_ts_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ts_no_context = np.std(no_context_ts_rewards_per_experiment, axis=0)\n",
    "\n",
    "mean_cum_reward_ucb_context = np.mean(context_ucb_rewards_per_experiment, axis=0)\n",
    "mean_cum_reward_ucb_no_context = np.mean(no_context_ucb_rewards_per_experiment, axis=0)\n",
    "\n",
    "std_cum_reward_ucb_context = np.std(context_ucb_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ucb_no_context = np.std(no_context_ucb_rewards_per_experiment, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT TS REWARD IST\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts_context, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts_context - std_cum_reward_ts_context, mean_cum_reward_ts_context + std_cum_reward_ts_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward, context')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT TS REWARD IST\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts_no_context, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts_no_context - std_cum_reward_ts_no_context, mean_cum_reward_ts_no_context + std_cum_reward_ts_no_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward, no context')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT TS REWARD CUM\n",
    "plt.ylabel(\"Cumulative Reward, context\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(context_ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts_context)), np.cumsum(mean_cum_reward_ts_context) - std_cum_reward_ts_context, np.cumsum(mean_cum_reward_ts_context) + std_cum_reward_ts_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT TS REWARD CUM\n",
    "plt.ylabel(\"Cumulative Reward, no context\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(no_context_ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts_no_context)), np.cumsum(mean_cum_reward_ts_no_context) - std_cum_reward_ts_no_context, np.cumsum(mean_cum_reward_ts_no_context) + std_cum_reward_ts_no_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {feature: envs[feature].optimal for feature in ['00', '01', '10', '11']}\n",
    "opt = sum(opts.values())\n",
    "\n",
    "#CONTEXT REGRET CUM\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret, context\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO CONTEXT REGRET CUM\n",
    "\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret, no context\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT REGRET IST\n",
    "mean_inst_regret_ts = opt - np.mean(np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret, context\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT REGRET IST\n",
    "mean_inst_regret_ts = opt - np.mean(np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret, no context\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
