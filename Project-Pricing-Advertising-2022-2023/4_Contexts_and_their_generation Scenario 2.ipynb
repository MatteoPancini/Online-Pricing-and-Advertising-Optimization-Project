{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts and their generation\n",
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.User_Classes import UserClass\n",
    "from p4.Multi_TS_Learner import Multi_TS_Learner\n",
    "from p4.Multi_UCB_Learner import Multi_UCB_Learner\n",
    "\n",
    "from p4.feature_based_environment import Environment\n",
    "from p4.ContextManager import ContextManager, ContextOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")\n",
    "uc2 = UserClass(name = \"C2\")\n",
    "uc3 = UserClass(name = \"C3\")\n",
    "\n",
    "user_classes = [uc1, uc2, uc3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "envs = {feature: Environment(feature) for feature in ['00', '01', '10', '11']}\n",
    "opts = {feature: envs[feature].optimal for feature in ['00', '01', '10', '11']}\n",
    "n_experiments = 2\n",
    "T = 100\n",
    "prices = [50, 100, 150, 200, 250]\n",
    "bids = np.linspace(0.01, 3.0, 100)\n",
    "\n",
    "no_context_ts_rewards_per_experiment = []\n",
    "context_ts_rewards_per_experiment = []\n",
    "\n",
    "no_context_ucb_rewards_per_experiment = []\n",
    "context_ucb_rewards_per_experiment = []\n",
    "\n",
    "for e in range(0, n_experiments):\n",
    "    context_TS = ContextOptimizer(Multi_TS_Learner)\n",
    "    nocontext_TS = Multi_TS_Learner(bids, prices)\n",
    "\n",
    "    context_UCB = ContextOptimizer(Multi_UCB_Learner)\n",
    "    nocontext_UCB = Multi_UCB_Learner(bids, prices)\n",
    "\n",
    "    for t in range(0, T):\n",
    "        if t % 10 == 0:\n",
    "            print(f\"{t} of experiment {e}\")\n",
    "\n",
    "        #context-generation using TS\n",
    "        pulled_arms_per_feature = context_TS.pull_arms()\n",
    "        optimizer_update_input = {}\n",
    "        for feature in pulled_arms_per_feature.keys(): #.keys ritorna 00 01 10 11\n",
    "            optimizer_update_input[feature] = pulled_arms_per_feature[feature] + envs[feature].round(*pulled_arms_per_feature[feature]) #ad ogni contesto assegno l'arm pullata sommata al round (perch√© la somma al round?)\n",
    "        context_TS.update(optimizer_update_input) #e alla fine faccio l'update\n",
    "\n",
    "        #context-generation using UCB\n",
    "        pulled_arms_per_feature = context_UCB.pull_arms()\n",
    "        optimizer_update_input = {}\n",
    "        for feature in pulled_arms_per_feature.keys():\n",
    "            optimizer_update_input[feature] = pulled_arms_per_feature[feature] + envs[feature].round(*pulled_arms_per_feature[feature])\n",
    "        context_UCB.update(optimizer_update_input)\n",
    "\n",
    "\n",
    "        #TS with no context generation\n",
    "        pulled_arms = nocontext_TS.pull_arms()\n",
    "        total_conversions = 0\n",
    "        total_n_clicks = 0\n",
    "        total_cum_cost = 0\n",
    "        total_reward = 0\n",
    "        for feature in ['00', '01', '10', '11']:\n",
    "            n_conversions, n_clicks, cum_cost, reward = envs[feature].round(*pulled_arms)\n",
    "            total_conversions += n_conversions\n",
    "            total_n_clicks += n_clicks\n",
    "            total_cum_cost += cum_cost\n",
    "            total_reward += reward\n",
    "        nocontext_TS.update(*pulled_arms, total_conversions, total_n_clicks, total_cum_cost, total_reward)\n",
    "\n",
    "        # UCB\n",
    "        pulled_arms = nocontext_UCB.pull_arms()\n",
    "        total_conversions = 0\n",
    "        total_n_clicks = 0\n",
    "        total_cum_cost = 0\n",
    "        total_reward = 0\n",
    "        for feature in ['00', '01', '10', '11']:\n",
    "            n_conversions, n_clicks, cum_cost, reward = envs[feature].round(*pulled_arms)\n",
    "            total_conversions += n_conversions\n",
    "            total_n_clicks += n_clicks\n",
    "            total_cum_cost += cum_cost\n",
    "            total_reward += reward\n",
    "        nocontext_UCB.update(*pulled_arms, total_conversions, total_n_clicks, total_cum_cost, total_reward)\n",
    "\n",
    "    context_ts_rewards_per_experiment.append(context_TS.collected_rewards)\n",
    "    no_context_ts_rewards_per_experiment.append(nocontext_TS.collected_rewards)\n",
    "\n",
    "    context_ucb_rewards_per_experiment.append(context_UCB.collected_rewards)\n",
    "    no_context_ucb_rewards_per_experiment.append(nocontext_UCB.collected_rewards)\n",
    "\n",
    "mean_cum_reward_ts_context = np.mean(context_ts_rewards_per_experiment, axis=0)\n",
    "mean_cum_reward_ts_no_context = np.mean(no_context_ts_rewards_per_experiment, axis=0)\n",
    "\n",
    "std_cum_reward_ts_context = np.std(context_ts_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ts_no_context = np.std(no_context_ts_rewards_per_experiment, axis=0)\n",
    "\n",
    "mean_cum_reward_ucb_context = np.mean(context_ucb_rewards_per_experiment, axis=0)\n",
    "mean_cum_reward_ucb_no_context = np.mean(no_context_ucb_rewards_per_experiment, axis=0)\n",
    "\n",
    "std_cum_reward_ucb_context = np.std(context_ucb_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ucb_no_context = np.std(no_context_ucb_rewards_per_experiment, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT TS REWARD IST\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts_context, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts_context - std_cum_reward_ts_context, mean_cum_reward_ts_context + std_cum_reward_ts_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward, context')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT TS REWARD IST\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts_no_context, 'b', label='TS')\n",
    "plt.fill_between(range(1, T+1), mean_cum_reward_ts_no_context - std_cum_reward_ts_no_context, mean_cum_reward_ts_no_context + std_cum_reward_ts_no_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Istantaneous Reward, no context')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT TS REWARD CUM\n",
    "plt.ylabel(\"Cumulative Reward, context\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(context_ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts_context)), np.cumsum(mean_cum_reward_ts_context) - std_cum_reward_ts_context, np.cumsum(mean_cum_reward_ts_context) + std_cum_reward_ts_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT TS REWARD CUM\n",
    "plt.ylabel(\"Cumulative Reward, no context\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Cumulative reward over time\")\n",
    "plt.plot(np.cumsum(np.mean(no_context_ts_rewards_per_experiment, axis=0)), 'b', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_reward_ts_no_context)), np.cumsum(mean_cum_reward_ts_no_context) - std_cum_reward_ts_no_context, np.cumsum(mean_cum_reward_ts_no_context) + std_cum_reward_ts_no_context, alpha=0.2, color='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {feature: envs[feature].optimal for feature in ['00', '01', '10', '11']}\n",
    "opt = sum(opts.values())\n",
    "\n",
    "#CONTEXT REGRET CUM\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret, context\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO CONTEXT REGRET CUM\n",
    "\n",
    "mean_cum_regret_ts = np.mean(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "std_cum_regret_ts = np.std(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "# Plot mean and standard deviation\n",
    "plt.plot(np.cumsum(mean_cum_regret_ts), 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_cum_regret_ts)), np.cumsum(mean_cum_regret_ts) - std_cum_regret_ts, np.cumsum(mean_cum_regret_ts) + std_cum_regret_ts, alpha=0.2, color='r')\n",
    "plt.ylabel(\"Cumulative Regret, no context\")\n",
    "plt.xlabel(\"Time\")\n",
    "#plt.plot(np.cumsum(opt - np.array(mean_cum_reward_ts), axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT REGRET IST\n",
    "mean_inst_regret_ts = opt - np.mean(np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret, context\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO CONTEXT REGRET IST\n",
    "mean_inst_regret_ts = opt - np.mean(np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "std_inst_regret_ts = np.std(opt - np.array(no_context_ts_rewards_per_experiment), axis=0)\n",
    "\n",
    "plt.ylabel(\"Istantaneous Regret, no context\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.plot(mean_inst_regret_ts, 'r', label='TS')\n",
    "plt.fill_between(range(len(mean_inst_regret_ts)), mean_inst_regret_ts - std_inst_regret_ts, mean_inst_regret_ts + std_inst_regret_ts, alpha=0.2, color='r')\n",
    "#plt.plot(np.mean(opt - np.array(ts_rewards_per_experiment), axis=0), 'r', label='TS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
