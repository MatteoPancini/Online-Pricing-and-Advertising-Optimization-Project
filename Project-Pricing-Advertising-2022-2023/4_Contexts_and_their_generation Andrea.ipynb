{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AR] Contexts and their generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case in which there are three classes of users (C1, C2, and C3), and no information about the advertising and pricing curves is known beforehand. Consider two scenarios. \n",
    "\n",
    "In the first one, the structure of the contexts is known beforehand. Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "In the second scenario, the structure of the contexts is not known beforehand and needs to be learnt from data. \n",
    "\n",
    "Important remark: the learner does not know how many contexts there are, while it can only observe the features and data associated with the features. \n",
    "\n",
    "Apply the GP-UCB and GP-TS algorithms when using GPs to model the two advertising curves paired with a context generation algorithm, reporting the plots with the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward. \n",
    "\n",
    "Apply the context generation algorithms every two weeks of the simulation. \n",
    "Compare the performance of the two algorithms --- the one used in the first scenario with the one used in the second scenario. \n",
    "\n",
    "Furthermore, in the second scenario, run the GP-UCB and GP-TS algorithms without context generation, and therefore forcing the context to be only one for the entire time horizon, and compare their performance with the performance of the previous algorithms used for the second scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "from User_Classes import UserClass\n",
    "from UCB import *\n",
    "from tqdm import tqdm\n",
    "from Learner import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Pricing_Environment import *\n",
    "from Advertising_Environment import *\n",
    "from TS_Learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")\n",
    "uc2 = UserClass(name = \"C2\")\n",
    "uc3 = UserClass(name = \"C3\")\n",
    "\n",
    "user_classes = [uc1, uc2, uc3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_Pricing_4(): #m\n",
    "    def __init__(self, n_arms):\n",
    "        self.classes = [\n",
    "            UserClass(name = 'C1'),\n",
    "            UserClass(name = 'C2'),\n",
    "            UserClass(name = 'C3')\n",
    "        ]\n",
    "        self.prices = [50, 100, 150, 200, 250]\n",
    "        self.time = 0\n",
    "        self.n_arms = n_arms\n",
    "        self.p1 = self.classes[0].get_conversion_probabilities()\n",
    "        self.p2 = self.classes[1].get_conversion_probabilities()\n",
    "        self.p3 = self.classes[2].get_conversion_probabilities()\n",
    "        self.ad_env = Advertising_Environment()\n",
    "\n",
    "    def get_conversion_price_probability(self, class_index, price_index):\n",
    "        prob = self.classes[class_index].get_conversion_probabilities()[price_index]\n",
    "        return prob\n",
    "\n",
    "    def round(self, class_index, price_index, bid=1):\n",
    "        prices = [50,100,150,200,250]\n",
    "        clicks = self.ad_env.generate_observations(noise_std_clicks=0, bid=bid, index=class_index)\n",
    "        conversion_prob = np.random.binomial(1, self.get_conversion_price_probability(0, price_index))\n",
    "        margin = prices[price_index] - (prices[price_index]/100)*30\n",
    "        costs = self.ad_env.get_total_cost(noise_std_cost=0, bid=bid, index=class_index)\n",
    "        reward = clicks * conversion_prob * margin - costs\n",
    "        self.time += 1\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(user_class, bid, price):\n",
    "    mean_per_bid = user_class.get_click_bids(bid) * (user_class.get_conversion_per_price(price) * calculate_margin(price)) - user_class.get_click_bids(bid) * user_class.get_cost_per_click(bid)    #sample_cost\n",
    "    return mean_per_bid\n",
    "\n",
    "class BiddingEnvironment:\n",
    "    def __init__(self, bids, sigma, user_classes, price, n_arms):\n",
    "        self.bids = bids\n",
    "        self.means = [\n",
    "            self.initialize_means(user_class=user_classes[0], bids=bids, price=price),\n",
    "            self.initialize_means(user_class=user_classes[1], bids=bids, price=price),\n",
    "            self.initialize_means(user_class=user_classes[2], bids=bids, price=price)\n",
    "        ]\n",
    "        self.sigmas = np.ones(len(bids)) * sigma\n",
    "        self.price = price\n",
    "        self.n_arms = n_arms\n",
    "\n",
    "    def initialize_means(self, user_class, bids, price):\n",
    "        means = np.zeros(len(bids))\n",
    "        for i in range(len(means)):\n",
    "            means[i] = fun(user_class, bids[i], price)\n",
    "        return means\n",
    "\n",
    "    def round(self, pulled_arm):\n",
    "        print(pulled_arm)\n",
    "        print(self.means[pulled_arm])\n",
    "        return np.random.normal(self.means[pulled_arm], self.sigmas[pulled_arm])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit_algorithms.Learner import *\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class GPTS_Learner(Learner):\n",
    "    def __init__(self, n_arms, arms):\n",
    "        super().__init__(n_arms)\n",
    "        self.arms = arms\n",
    "        self.means = np.zeros(self.n_arms)\n",
    "        self.sigmas = np.ones(self.n_arms)*10\n",
    "        self.pulled_arms = []\n",
    "        alpha = 10.0\n",
    "        #kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n",
    "        #kernel_BlueCow\n",
    "        kernel = C(100, (100, 1e6)) * RBF(1, (1e-1, 1e1))\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, alpha = alpha**2, normalize_y=True, n_restarts_optimizer= 9)\n",
    "\n",
    "#Override to update the value of the pulled arms list\n",
    "    def update_observations(self, arm_idx, reward):\n",
    "        super().update_observations(arm_idx, reward)\n",
    "        self.pulled_arms.append(self.arms[arm_idx])\n",
    "#update the means and sigmas with the new predictions\n",
    "    def update_model(self):\n",
    "        x = np.atleast_2d(self.pulled_arms).T\n",
    "        y = self.collected_rewards\n",
    "        self.gp.fit(x,y)\n",
    "        self.means, self.sigmas = self.gp.predict(np.atleast_2d(self.arms).T, return_std=True)\n",
    "        self.sigmas = np.maximum(self.sigmas, 1e-2)\n",
    "#calls both update lists\n",
    "    def update(self, pulled_arm, reward):\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm, reward)\n",
    "        self.update_model()\n",
    "#returns the index of the max value drawn from the arm normal distribution\n",
    "    def pull_arm(self):\n",
    "        sampled_values = np.random.normal(self.means, self.sigmas)\n",
    "        return np.argmax(sampled_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandit_algorithms.Learner import *\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class GPUCB_Learner(Learner):\n",
    "    def __init__(self, n_arms, arms):\n",
    "        super().__init__(n_arms)\n",
    "        self.arms = arms\n",
    "        self.empirical_means = np.zeros(n_arms)\n",
    "        self.confidence = np.array([np.inf] * n_arms)\n",
    "        self.pulled_arms = []\n",
    "        alpha = 10.0\n",
    "        # kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n",
    "        # kernel_BlueCow\n",
    "        kernel = C(100, (100, 1e6)) * RBF(1, (1e-1, 1e1))\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha ** 2, normalize_y=True, n_restarts_optimizer=9)\n",
    "\n",
    "    def update_observations(self, arm_idx, reward):\n",
    "        super().update_observations(arm_idx, reward)\n",
    "        self.pulled_arms.append(self.arms[arm_idx])\n",
    "    def update_model(self):\n",
    "        x = np.atleast_2d(self.pulled_arms).T\n",
    "        y = self.collected_rewards\n",
    "        self.gp.fit(x,y)\n",
    "        self.means, self.sigmas = self.gp.predict(np.atleast_2d(self.arms).T, return_std=True)\n",
    "        self.sigmas = np.maximum(self.sigmas, 1e-2)\n",
    "    def pull_arm(self):\n",
    "        upper_conf = self.empirical_means + self.confidence\n",
    "        return np.random.choice(np.where(upper_conf == upper_conf.max())[0])\n",
    "\n",
    "    def update(self, pull_arm, reward):\n",
    "        self.t += 1\n",
    "        self.empirical_means[pull_arm] = (self.empirical_means[pull_arm] * (self.t - 1) + reward) / self.t\n",
    "        for a in range(self.n_arms):\n",
    "            n_samples = len(self.rewards_per_arm[a])\n",
    "            self.confidence[a] = (2 * np.log(self.t) / n_samples) ** 0.5 if n_samples > 0 else np.inf\n",
    "        self.update_observations(pull_arm, reward)\n",
    "        self.update_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First scenario\n",
    "\n",
    "Structure of the context is known: we know the three user classes and their two features\n",
    "\n",
    "GP-UCB + GP-TS to model the two adv curves: plot avg + std of cumulative regret/reward and istantaneous ones\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "- Define number of actions (n_arms), possible prices, margins, time horizon (T) [...]\n",
    "- Define user classes (C1, C2, C3) and their characteristics\n",
    "- Calculate the optimal price and its index for each class using an optimization algorithm (like exhaustive search over possible prices).\n",
    "\n",
    "- Define the number of experiments (#experiments) you want to run.\n",
    "\n",
    "- Initialize empty lists to store the final rewards and context splits for each experiment.\n",
    "\n",
    "- Loop over each experiment:\n",
    "\n",
    "  - Initialize GP-TS and GP-UCB learners for each context.\n",
    "  - Set up an environment with the parameters and classes defined above.\n",
    "  - Perform context splitting to determine initial contexts. Here, you will use GP-TS and GP-UCB to predict outcomes for different bids and prices, then update your models based on observed outcomes.\n",
    "  - For each context, repeat the context splitting process twice more to refine the contexts. Each time, update your GP models with the observed outcomes.\n",
    "  - Calculate the aggregate rewards and store them.\n",
    "  - Record the contexts after each split for further analysis.\n",
    "- After all experiments are complete, plot the average rewards and regrets over time for both GP-TS and GP-UCB, using the stored rewards and the calculated optimal rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(user_classes)):\n\u001b[0;32m     37\u001b[0m         \u001b[39m#GP Thompson Sampling\u001b[39;00m\n\u001b[0;32m     38\u001b[0m         pulled_arm \u001b[39m=\u001b[39m gpts_learners[c]\u001b[39m.\u001b[39mpull_arm()\n\u001b[1;32m---> 39\u001b[0m         reward \u001b[39m=\u001b[39m env_adv\u001b[39m.\u001b[39;49mround(pulled_arm)\n\u001b[0;32m     40\u001b[0m         gpts_learners[c]\u001b[39m.\u001b[39mupdate(pulled_arm, reward)\n\u001b[0;32m     42\u001b[0m gpts_rewards_per_experiment\u001b[39m.\u001b[39mappend(gpts_learners[c]\u001b[39m.\u001b[39mcollected_rewards)\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mBiddingEnvironment.round\u001b[1;34m(self, pulled_arm)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m, pulled_arm):\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(pulled_arm)\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeans[pulled_arm])\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans[pulled_arm], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas[pulled_arm])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "n_arms = 10\n",
    "\n",
    "#Creating the pricing environment\n",
    "env_pr = Environment_Pricing_4(n_arms=n_arms)\n",
    "\n",
    "#Defining adv variables and creating the adv env\n",
    "n_arms = 100\n",
    "min_bid = 0.01\n",
    "max_bid = 3.0\n",
    "bids = np.linspace(min_bid, max_bid, n_arms)\n",
    "sigma = 200\n",
    "\n",
    "prices = env_pr.prices\n",
    "optimal_prices = [200, 200, 150]\n",
    " \n",
    "margins = [i*0.7 for i in range(50, 251, 50)]\n",
    "T = 365\n",
    "return_time = 30\n",
    "table = None\n",
    "\n",
    "simulation_interval = 14 #every two weeks, run the context generation algorithm\n",
    "\n",
    "ts_final_rewards = []\n",
    "split_record = []\n",
    "\n",
    "n_experiments = 10\n",
    "gpts_rewards_per_experiment = []\n",
    "\n",
    "\n",
    "#for each experiment, for each day, for each class\n",
    "for e in range(n_experiments):\n",
    "    env_adv = BiddingEnvironment(bids=bids, sigma = sigma, user_classes=user_classes, price=100, n_arms=n_arms) #price? for each price?\n",
    "    gpts_learners = [GPTS_Learner(n_arms, bids), GPTS_Learner(n_arms, bids), GPTS_Learner(n_arms, bids)]\n",
    "\n",
    "    for t in range(T):\n",
    "        for c in range(len(user_classes)):\n",
    "            #GP Thompson Sampling\n",
    "            pulled_arm = gpts_learners[c].pull_arm()\n",
    "            reward = env_adv.round(pulled_arm)\n",
    "            gpts_learners[c].update(pulled_arm, reward)\n",
    "\n",
    "    gpts_rewards_per_experiment.append(gpts_learners[c].collected_rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
