{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Learning for Pricing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case in which all the users belong to class C1. Assume that the curves related to the advertising part of the problem are known, while the curve related to the pricing problem is not. Apply the UCB1 and TS algorithms, reporting the plots of the average (over a sufficiently large number of runs) value and standard deviation of the cumulative regret, cumulative reward, instantaneous regret, and instantaneous reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "from User_Classes import UserClass\n",
    "from UCB import *\n",
    "from Learner import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Pricing_Environment import *\n",
    "from TS_Learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc1 = UserClass(name = \"C1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "[[ 1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00408646e+04 -1.00378646e+04]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "b <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m ucb_learner \u001b[39m=\u001b[39m UCB(n_arms\u001b[39m=\u001b[39mn_arms)\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,T):\n\u001b[0;32m     22\u001b[0m     \u001b[39m#Thompson sampling\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     pulled_arm \u001b[39m=\u001b[39m ts_learner\u001b[39m.\u001b[39;49mpull_arm()\n\u001b[0;32m     24\u001b[0m     reward \u001b[39m=\u001b[39m env_pr\u001b[39m.\u001b[39mround(class_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, price_index\u001b[39m=\u001b[39mpulled_arm, bid\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n\u001b[0;32m     25\u001b[0m     ts_learner\u001b[39m.\u001b[39mupdate(pulled_arm, reward)\n",
      "File \u001b[1;32mc:\\Users\\Andrea\\Documents\\GitHub\\OLA_2023_Private\\Project-Pricing-Advertising-2022-2023\\TS_Learner.py:21\u001b[0m, in \u001b[0;36mTS_Learner.pull_arm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpull_arm\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_parameters)\n\u001b[1;32m---> 21\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mbeta(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta_parameters[:,\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta_parameters[:,\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m idx\n",
      "File \u001b[1;32mmtrand.pyx:481\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.beta\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:600\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:505\u001b[0m, in \u001b[0;36mnumpy.random._common.cont_broadcast_2\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:389\u001b[0m, in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: b <= 0"
     ]
    }
   ],
   "source": [
    "n_arms = 5\n",
    "\n",
    "p = uc1.get_conversion_probabilities()\n",
    "env = Environment_Pricing(n_arms=n_arms, p = p)\n",
    "prices = env.prices\n",
    "\n",
    "opt = p[0] #optimal arm is the one with the highest probability of success\n",
    "\n",
    "T = 365 #time steps for each experiment \n",
    "\n",
    "n_experiments = 1000\n",
    "\n",
    "ts_rewards_per_experiment = [] #list to store the collected rewards for TS_Learner over each experiment\n",
    "ucb_reward_per_experiment = [] #list to store the collected rewards for Greedy_Learner over each experiment\n",
    "\n",
    "# Loop over the experiments\n",
    "for e in range (0, n_experiments):\n",
    "    env_pr = Environment_Pricing(n_arms=n_arms, p = p)\n",
    "    ts_learner = TS_Learner(n_arms=n_arms)\n",
    "    ucb_learner = UCB(n_arms=n_arms)\n",
    "    for t in range(0,T):\n",
    "        #Thompson sampling\n",
    "        pulled_arm = ts_learner.pull_arm()\n",
    "        reward = env_pr.round(class_index=0, price_index=pulled_arm, bid=0.25)\n",
    "        ts_learner.update(pulled_arm, reward)\n",
    "\n",
    "        # Greedy\n",
    "        pulled_arm = ucb_learner.pull_arm()\n",
    "        reward = env_pr.round(class_index=0, price_index = pulled_arm, bid=0.25)\n",
    "        ucb_learner.update(pulled_arm, reward)\n",
    "\n",
    "\n",
    "    ts_rewards_per_experiment.append(ts_learner.collected_rewards)\n",
    "    ucb_reward_per_experiment.append(ucb_learner.collected_rewards)\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(0)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt - ts_rewards_per_experiment, axis = 0)), 'r')\n",
    "plt.plot(np.cumsum(np.mean(opt - gr_reward_per_experiment, axis = 0)), 'g')\n",
    "plt.legend([\"TS\", \"Greedy\"])\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Compute the mean and standard deviation of the cumulative reward at each round\n",
    "mean_cum_reward_ts = np.mean(ts_rewards_per_experiment, axis=0)\n",
    "std_cum_reward_ts = np.std(ts_rewards_per_experiment, axis=0)\n",
    "\n",
    "mean_cum_reward_ucb = np.mean(ucb_reward_per_experiment, axis=0)\n",
    "std_cum_reward_ucb = np.std(ucb_reward_per_experiment, axis=0)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ts, label='TS')\n",
    "\n",
    "plt.plot(range(1, T+1), mean_cum_reward_ucb, label='UCB')\n",
    "\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
